{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(\n",
    "    aws_access_key_id='ASIAT2RS5RDCI4Z4WPNU',\n",
    "    aws_secret_access_key='aWsxH2sosn4+Ms/2qkr1HRdac71bbP2dOubCY+Sv',\n",
    "    aws_session_token='IQoJb3JpZ2luX2VjEEQaCXVzLXdlc3QtMiJHMEUCIQCdT3m/2bIflSZ25kTBOzJJSpbs2i8xYTUxZdTQl+IvQwIgAplfDY7Lq2zsBebnk1sarVYSBqXrZkFIvLo1hgWKVhYquAII3f//////////ARACGgwyNjMxNzM1NDAwMzYiDD33+Od4dTMNuD3PpSqMAoFHLjwgTkRFZcrVw6DTrr4qYOTBQikBZlCdnsGbojPlk2GoWclgjuRRS4o1IFmHKMQp1l6e2kQAtrmFNBCU0bO8DQe3fpjbeSdwfFfHfbNxj36gfFvnG42mxNvlMWRWxqfggtYu6TWzmWLueBzH3UM1D8EmqFA5h1izAXaV9pek2IF/RcjqIdNGqbmiEk4FmLGQ7e5eUV4TjvVeLCDibuC7pFmU2Gq9AIxZLDeJxLgzRNi/DsgZFuh40esprzAP2No0MQuR+Exd+MGYl6i+P3owc3dFGGUGG+8sUnhPZmseoe3whMvs7tQZMd1JaZrAJgXNXgIleghZLoACnTCzrxGGvnFd88t3J3nO8d4wltmIugY6nQGaZe1SLR9pUvVR1leKES+wbaPjxH8U4JxwtfE5kORWsQiSr9dRBGEo4u7C0V5nzWOGShc5k0Zmo0hqMdHFhvUNbYsSTLvpvOt9en50dAxfILReAEk4PQMty9dCh4Fog3D66uyNY6CXvWeo8/zRA7NqE9Nnz772SqfQf+Q3b98Eo1Qoj3X7tabVAUDO0n85Qkv/Mj8uXDLh02vihvE0',\n",
    "    region_name='us-east-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = session.client('emr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster creado con éxito: j-1DOVP13JGOGV9\n"
     ]
    }
   ],
   "source": [
    "response = client.run_job_flow(\n",
    "    Name=\"2024-II\",\n",
    "    ReleaseLabel='emr-7.3.0',\n",
    "    LogUri='s3://big-data-topicos/EMR/logs/',\n",
    "    Instances={\n",
    "        'KeepJobFlowAliveWhenNoSteps': True,\n",
    "        'TerminationProtected': False,\n",
    "        'InstanceGroups': [\n",
    "            {\n",
    "                'Name': 'Master',\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'MASTER',\n",
    "                'InstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 1\n",
    "            },\n",
    "            {\n",
    "                'Name': 'Core',\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'CORE',\n",
    "                'InstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 2\n",
    "            },\n",
    "            {\n",
    "                'Name': 'Task',\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'TASK',\n",
    "                'InstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 1\n",
    "            }\n",
    "        ],\n",
    "        'Ec2KeyName': 'emr'\n",
    "    },\n",
    "    Applications=[\n",
    "        {'Name': 'Spark'},\n",
    "        {'Name': 'Hadoop'},\n",
    "        {'Name': 'Hive'},\n",
    "        {'Name': 'JupyterHub'},\n",
    "        {'Name': 'Livy'},\n",
    "        {'Name': 'Zeppelin'},\n",
    "        {'Name': 'TensorFlow'},\n",
    "        {'Name': 'Hue'},\n",
    "        {'Name': 'Tez'},\n",
    "        {'Name': 'Zookeeper'}\n",
    "    ],\n",
    "    Steps=[\n",
    "        {\n",
    "            'Name': 'Install Dependencies',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'pip3', 'install', 'pandas', 'sodapy', 'boto3', 's3fs', 'fsspec', 'mysql-connector-python'\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'Name': 'Run Python Script to Consume API',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'bash', '-c', \n",
    "                    'aws s3 cp s3://big-data-topicos/bigdata/ingest.py /home/hadoop/ingest.py && python3 /home/hadoop/ingest.py'\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'Name': 'Load DB',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'bash', '-c', \n",
    "                    'aws s3 cp s3://big-data-topicos/COVID/load_s3.py /home/hadoop/load_s3.py && python3 /home/hadoop/load_s3.py'\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'Name': 'ETL',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'bash', '-c', \n",
    "                    'aws s3 cp s3://big-data-topicos/COVID/ETL.py /home/hadoop/ETL.py && spark-submit /home/hadoop/ETL.py'\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'Name': 'DF',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'bash', '-c', \n",
    "                    'aws s3 cp s3://big-data-topicos/COVID/dataframes.py /home/hadoop/dataframes.py && spark-submit /home/hadoop/dataframes.py'\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'Name': 'SparkSQL',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'bash', '-c', \n",
    "                    'aws s3 cp s3://big-data-topicos/COVID/Sparksql.py /home/hadoop/Sparksql.py && spark-submit /home/hadoop/Sparksql.py'\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "    ],\n",
    "    VisibleToAllUsers=True,\n",
    "    ServiceRole='EMR_DefaultRole',\n",
    "    JobFlowRole='EMR_EC2_DefaultRole',\n",
    "    AutoScalingRole='EMR_AutoScaling_DefaultRole'\n",
    ")\n",
    "\n",
    "print(\"Cluster creado con éxito:\", response['JobFlowId'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
